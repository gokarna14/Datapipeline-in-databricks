{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"1b5a3f56-00f2-4575-9142-702bc9d30e48","showTitle":false,"title":""}},"outputs":[],"source":["import requests\n","from pyspark.sql.functions import col\n","import time\n","import uuid\n","from datetime import datetime\n","from pyspark.sql.types import DateType\n","from pyspark.sql.types import TimestampType"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"83997cfd-6cd3-4de9-9d3a-7cf7e7fdf960","showTitle":false,"title":""}},"source":["# Utility Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"07a629d4-1941-40ef-9d70-7a5849ccb552","showTitle":false,"title":""}},"outputs":[],"source":["class Utility:\n","    def convert_int_to_float(data):\n","        if isinstance(data, int):\n","            return float(data)\n","        elif isinstance(data, list):\n","            return [Utility.convert_int_to_float(item) for item in data]\n","        elif isinstance(data, dict):\n","            return {k: Utility.convert_int_to_float(v) for k, v in data.items()}\n","        else:\n","            return data\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"cabe5068-9fa8-45ce-94f0-76b168b7598f","showTitle":false,"title":""}},"source":["# Get API"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"c8f29875-bad4-4a89-8325-5710c0d96157","showTitle":false,"title":""}},"outputs":[],"source":["class Weather:\n","    \n","    raw_schema = \"RAW\"\n","    raw_table = \"raw_weather\"\n","    \n","    def get_and_load_raw(createdBy : str = 'Gokarna Adhiarki', selectedCities : list[str]= ['Kathmandu', 'Pokhara', 'Gorkha', 'Biratnagar', 'Kirtipur'] ):\n","        cities = spark.read.load(\"dbfs:/FileStore/tables/Cities\")\n","        cities = cities.filter(col('name').isin(selectedCities)).select('name', 'lat', 'lon')\n","        \n","        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {Weather.raw_schema};\")\n","        \n","        \n","        res = []\n","        id = LogTable.load('RAW', 'STARTED', 'raw.raw_weather') \n","        for i, r in cities.toPandas().iterrows():\n","            data = Weather.get_response(*r)\n","            toLoad = {\n","                'created_on' : str(datetime.now()),\n","                'created_by' : createdBy,\n","                'data' : data,\n","                'id': id\n","            }  \n","            \n","            json_rdd = spark.sparkContext.parallelize([toLoad])\n","            df = spark.read.json(json_rdd)\n","            \n","            LogTable.load('RAW', 'EXTRACTING', 'raw.raw_weather')\n","            df.write.format('delta').mode('append').option(\"mergeSchema\", \"true\").saveAsTable(f\"{Weather.raw_schema}.{Weather.raw_table}\")\n","\n","        LogTable.load('RAW', 'COMPLETED', 'raw.raw_weather')\n","        \n","        return \"Successfully Loaded !\"\n","        \n","    def get_response(cityName : str, lat : float, lon : float) -> dict:\n","        api_key = '*********'\n","        url = f\"https://api.openweathermap.org/data/2.5/weather?lat={lat}&lon={lon}&appid={api_key}&units=metric\"\n","        response = requests.get(url)\n","        data = response.json()\n","        data['city'] = cityName\n","\n","        return Utility.convert_int_to_float(data)"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"7f33c70c-e7af-46b1-8783-ca0d814e1706","showTitle":false,"title":""}},"source":["# Log Maintain"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"aa131b23-d952-45b0-9b0f-4e3d2da875d8","showTitle":false,"title":""}},"outputs":[],"source":["class LogTable:\n","    schemaName = 'FACT'\n","    tableName = 'log_table'     \n","    def load(load_type, status, load_to_table_name : str, comments='', created_by='Gokarna Adhikari'):\n","        id = str(uuid.uuid4())\n","        LogTable.load_table(     \n","            LogTable.schemaName,\n","            LogTable.schemaName, \n","            id = id,\n","            load_type = load_type,\n","            table_name = load_to_table_name,\n","            process_start_time = str(datetime.now()),\n","            process_end_time = str(datetime.now()),\n","            status = status,\n","            comments = comments,\n","            start_date_time = str(datetime.now()),\n","            end_date_time = str(datetime.now()),\n","            created_on = str(datetime.now()),\n","            created_by = created_by\n","                      )\n","        return id\n","        \n","        \n","    def load_table(schema : str, table : str, **kwargs):\n","        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema};\")\n","        json_rdd = spark.sparkContext.parallelize([kwargs])\n","        df = spark.read.json(json_rdd)\n","\n","        # converting to compatible type\n","        timestamp_cols = [\"process_start_time\", \"process_end_time\", \"start_date_time\", \"end_date_time\", \"created_on\"]\n","        for col_name in timestamp_cols:\n","            df = df.withColumn(col_name, col(col_name).cast(TimestampType()))\n","\n","        df.write.format('delta').mode('append').option(\"mergeSchema\", \"true\").saveAsTable(f\"{schema}.{LogTable.tableName}\")\n","        \n","        return kwargs['id']\n"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":4008208679740076,"dataframes":["_sqldf"]},"pythonIndentUnit":4},"notebookName":"Load_Raw","widgets":{}},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
