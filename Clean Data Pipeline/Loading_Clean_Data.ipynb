{"cells":[{"cell_type":"code","source":["import requests\nfrom pyspark.sql.functions import col\nimport time\nimport uuid\nfrom datetime import datetime\nfrom pyspark.sql.types import DateType\nfrom pyspark.sql.types import TimestampType"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"75ef0139-ecb3-41c9-96ad-5398b5b9720b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Log Maintain"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"07cc9cce-8a12-4863-b7a5-4b62379b7502","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["class LogTable:\n    schemaName = 'FACT'\n    tableName = 'log_table'     \n    def load(load_type, status, load_to_table_name : str, comments='', created_by='Gokarna Adhikari'):\n        id = str(uuid.uuid4())\n        LogTable.load_table(     \n            LogTable.schemaName,\n            LogTable.schemaName, \n            id = id,\n            load_type = load_type,\n            table_name = load_to_table_name,\n            process_start_time = str(datetime.now()),\n            process_end_time = str(datetime.now()),\n            status = status,\n            comments = comments,\n            start_date_time = str(datetime.now()),\n            end_date_time = str(datetime.now()),\n            created_on = str(datetime.now()),\n            created_by = created_by\n                      )\n        return id\n        \n        \n    def load_table(schema : str, table : str, **kwargs):\n        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema};\")\n        json_rdd = spark.sparkContext.parallelize([kwargs])\n        df = spark.read.json(json_rdd)\n\n        # converting to compatible type\n        timestamp_cols = [\"process_start_time\", \"process_end_time\", \"start_date_time\", \"end_date_time\", \"created_on\"]\n        for col_name in timestamp_cols:\n            df = df.withColumn(col_name, col(col_name).cast(TimestampType()))\n\n        df.write.format('delta').mode('append').option(\"mergeSchema\", \"true\").saveAsTable(f\"{schema}.{LogTable.tableName}\")\n        \n        return kwargs['id']\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"37e0ed94-2360-47b5-9965-ff75d7cffa23","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Function -> Loading clean data into table"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7ed7a455-8015-441f-b348-67442c842f10","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df = spark.table('raw.raw_weather')\ndf = df.orderBy(df[\"created_on\"].desc()).limit(5)\ndf_clean = df.select(\n    col(\"data.dt\").alias(\"dt\"),\n    col(\"data.id\").cast(\"int\").alias(\"id\"),\n    col(\"data.city\").alias(\"city\"),\n    col(\"data.timezone\").cast(\"int\").alias(\"timezone\"),\n    col(\"data.sys.country\").alias(\"country\"),\n    col(\"data.coord.lat\").alias(\"lat\"),\n    col(\"data.coord.lon\").alias(\"lon\"),\n    col(\"data.main.temp\").alias(\"temp\"),\n    col(\"data.main.temp_min\").alias(\"temp_min\"),\n    col(\"data.main.temp_max\").alias(\"temp_max\"),\n    col(\"data.main.pressure\").alias(\"pressure\"),\n    col(\"data.main.humidity\").alias(\"humidity\"),\n    col(\"data.visibility\").alias(\"visibility\"),\n    col(\"data.wind.speed\").alias(\"speed\"),\n    col(\"data.wind.deg\").alias(\"deg\"),\n    col(\"data.wind.gust\").alias(\"gust\"),\n    col(\"id\").alias(\"load_run_id\"), #root level id\n    col(\"created_on\").alias(\"created_on\"),\n    col(\"created_by\").alias(\"created_by\")\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"b7154d88-0e9e-4706-b895-a50ef6b3b327","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def Clean_Data_Load():\n    LogTable.load('CLEAN', 'STARTED', 'raw.raw_weather')\n    table_name = 'clean_weather_data'\n    schema = 'CLEAN'\n    spark.sql(f'CREATE SCHEMA IF NOT EXISTS {schema}')\n    df_clean.write.format('delta').mode('append').option(\"mergeSchema\", \"true\").saveAsTable(f\"{schema}.{table_name}\")\n    LogTable.load('CLEAN', 'COMPLETED', 'raw.raw_weather')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"600f7eb5-c83a-4d27-a54f-95b66e9824b3","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Clean_Data_Load();"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"7c30ddc2-37d8-4649-bb06-c06a1a5cfb8a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# %sql\n# -- SELECT * FROM clean.clean_weather_data"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000,"implicitDf":true},"nuid":"e73a1bfc-1d34-49f0-bbc5-49ac80041ec3","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Loading_Clean_Data","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":-1,"dataframes":["_sqldf"]}},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
